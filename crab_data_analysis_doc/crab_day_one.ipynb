{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc6f0d3",
   "metadata": {},
   "source": [
    "# CRAB Day One "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3904d",
   "metadata": {},
   "source": [
    "### Table of content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d9510b",
   "metadata": {},
   "source": [
    "1. [CRAB in brief](#crab-in-brief)\n",
    "2. [Requests and access](#requests-and-access)\n",
    "3. [EOS](#eos)\n",
    "4. [SWAN](#swan)\n",
    "5. [PySpark on SWAN](#pyspark-on-swan)\n",
    "6. [HDFS Starter](#hdfs-starter)\n",
    "7. [LXPLUS](#lxplus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8cb4ab",
   "metadata": {},
   "source": [
    "### CRAB in brief"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b498d6",
   "metadata": {},
   "source": [
    "\n",
    "**CRAB**, short for the CMS Remote Analysis Builder, is a utility to submit CMSSW jobs to distributed computing resources(Grid). \n",
    "\n",
    "By using CRAB you will be able to:\n",
    "* Access CMS data and Monte-Carlo which are distributed to CMS aligned centres worldwide.\n",
    "* Exploit the CPU and storage resources at CMS aligned centres.\n",
    "\n",
    "To use CRAB to submit your CMSSW job to the Grid, you must meet some prerequisites:\n",
    "1. Get a Grid certificate and the registration to CMS VO\n",
    "2. Setup your certificate for LCG\n",
    "3. Test your grid certificate\n",
    "4. Test the code locally\n",
    "5. Validate a CMSSW config file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e179b2",
   "metadata": {},
   "source": [
    "### Requests and access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddfe918",
   "metadata": {},
   "source": [
    "1. Hadoop Cluster Access\n",
    "    * To do data analysis and be connected to the Hadoop cluster, you need to gain the Hadoop cluster access fisrt via the [CERN Service Portal.](https://cern.service-now.com/service-portal?id=service_element&name=Hadoop-Service)\n",
    "    * [Getting started with Hadoop](https://hadoop-user-guide.web.cern.ch/gettingstarted_md.html)\n",
    "    * [Using SPARK on Hadoop](https://hadoop-user-guide.web.cern.ch/spark/Using_Spark_on_Hadoop.html)\n",
    "2. CERNBox\n",
    "    * CERNBox is a place you can share your work projects. It is also required that you have your own CERNBox before using SWAN as it will be your SWAN home directory.\n",
    "    * Access to CERNBox [here.](https://cernbox.web.cern.ch/cernbox/) \n",
    "3. Office Key Request\n",
    "    * It is also important that you have your own key to the office. Request the key [here.](https://cern.service-now.com/service-portal?id=service_element&name=locks-keys-app-support)\n",
    "4. Bike Rental\n",
    "    * Rent a bike [here.](https://cern.service-now.com/service-portal?id=service_element&name=bicycle-rental)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0cc78a",
   "metadata": {},
   "source": [
    "### EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25882511",
   "metadata": {},
   "source": [
    "EOS provides a service for storing large amounts of physics data and user files, with a focus on interactive and batch analysis. [CERN EOS main page](https://eos-web.web.cern.ch/eos-web/). It is important to note that your files, folders, pictures that are saved in SWAN are stored in EOS where you can interact with them through your CERNBox home directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055d226",
   "metadata": {},
   "source": [
    "To check your EOS quota, follow this [instructions.](https://clouddocs.web.cern.ch/containers/tutorials/hdfs.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7455e3",
   "metadata": {},
   "source": [
    "### SWAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a86a9",
   "metadata": {},
   "source": [
    "SWAN (Service for Web based ANalysis) is a platform to perform interactive data analysis in the cloud. It uses your CERNBox as the home directory and you can access CERN experiments' and user data in CERN Cloud (EOS) there. [Read more about SWAN.](https://swan.web.cern.ch/swan/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b1e5d",
   "metadata": {},
   "source": [
    "Examples of how to use SWAN to do data analysis and visualization [here.](https://swan-gallery.web.cern.ch/basic/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73f25c8",
   "metadata": {},
   "source": [
    "### PySpark on SWAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5abcac",
   "metadata": {},
   "source": [
    "PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment. PySpark supports most of Sparkâ€™s features such as Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) and Spark Core. [Read more.](https://spark.apache.org/docs/latest/api/python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18827f3",
   "metadata": {},
   "source": [
    "Pre-requisites:\n",
    "1. *CERNBox*: Request [here](https://cern.service-now.com/service-portal?id=service_element&name=CERNBox-Service)\n",
    "2. *Hadoop Cluster Access*: Request [here](https://cern.service-now.com/service-portal?id=service_element&name=Hadoop-Service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f9214",
   "metadata": {},
   "source": [
    "How-to:\n",
    "1. Log in to SWAN directly from the [SWAN page](https://swan-k8s.cern.ch) or log in to your CERNBox then open SWAN Notebook from there.\n",
    "2. Create a new project and a new file.\n",
    "3. On the top panel, look for a *Star* button. It will appear there only if you already have the Hadoop cluster access. Configure needed library then click *connect*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30636129",
   "metadata": {},
   "source": [
    "Note:\n",
    "- The following variables are automatically initiated when connect to the Spark cluster:\n",
    "    * sc = [SparkContext](https://spark.apache.org/docs/3.2.0/api/java/org/apache/spark/SparkContext.html#:~:text=A%20SparkContext%20represents%20the%20connection,before%20creating%20a%20new%20one.)\n",
    "    * spark = [SparkSession](https://spark.apache.org/docs/3.2.0/api/java/org/apache/spark/sql/SparkSession.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f67b3",
   "metadata": {},
   "source": [
    "### HDFS Starter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d163a83c",
   "metadata": {},
   "source": [
    "Learn more about CERN HDFS [here.](https://clouddocs.web.cern.ch/containers/tutorials/hdfs.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dacf3f",
   "metadata": {},
   "source": [
    "#### Connect via command line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8ed4e2",
   "metadata": {},
   "source": [
    "1. Connect to [LXPLUS](#LXPLUS) using SSH. \n",
    "    * Machines running CentOS7 Linux 64 bit mode: @lxplus7.cern.ch \n",
    "    * Machines running CentOS8: @lxplus8.cern.ch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh [username]@lxplus7.cern.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7620c7d7",
   "metadata": {},
   "source": [
    "2. configure as indicated in [here](https://hadoop-user-guide.web.cern.ch/getstart/client_cvmfs.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0729f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first time setup\n",
    "source /cvmfs/sft.cern.ch/lcg/views/LCG_99/x86_64-centos7-gcc8-opt/setup.sh\n",
    "#if use spark\n",
    "source /cvmfs/sft.cern.ch/lcg/etc/hadoop-confext/hadoop-swan-setconf.sh analytix 3.2 spark3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cad592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kinit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d182fd",
   "metadata": {},
   "source": [
    "3. Explore the HDFS by looking up the path. [More path from CMS MONIT docs](https://cmsmonit-docs.web.cern.ch/MONIT/sources/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f27fe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b34bb",
   "metadata": {},
   "source": [
    "#### Connect via SWAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e91817",
   "metadata": {},
   "source": [
    "After connecting to the cluster, you can explore HDFS directly from SWAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7707095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd85de82",
   "metadata": {},
   "source": [
    "#### Create your own directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fcd7f8",
   "metadata": {},
   "source": [
    "You can have your own directory in the HDFS. The example shows the directory in case pf working under CMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir hdfs://analytix/cms/users/[username]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f2396",
   "metadata": {},
   "source": [
    "#### More commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c4ddec",
   "metadata": {},
   "source": [
    "Read the HDFS commands full document [here.](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d054dcb",
   "metadata": {},
   "source": [
    "1. Check size of the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -du -s -h hdfs://[path]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532bdbbe",
   "metadata": {},
   "source": [
    "2. Copy a file/ folder from HDFS to your EOS storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17277ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cp  hdfs://[path] file:${PWD}/[path]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a942b44c",
   "metadata": {},
   "source": [
    "Note that the PWD specifies your current directory in the [EOS](#EOS) space. You can check it by running the command on SWAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!$PWD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93121338",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b135ccfc",
   "metadata": {},
   "source": [
    "### LXPLUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9012d9aa",
   "metadata": {},
   "source": [
    "LXPLUS (Linux Public Login User Service) is the interactive logon service to Linux for all CERN users. The cluster LXPLUS consists of public machines provided by the IT Department for interactive work. [Read more here.](https://lxplusdoc.web.cern.ch/) LXPLUS can be accessed using SSH, short for [Secure Shell.](https://www.ucl.ac.uk/isd/what-ssh-and-how-do-i-use-it) The machines all have access to the [AFS](https://cern.service-now.com/service-portal?id=service_element&name=afs-service) file system for home directories, group space and some degree of scratch space, all of which are accessible through normal file system commands. In addition for physics and bulk data, the EOS and CASTOR services are avaialble. In particular EOS can be accessed as a filesystem through a fuse mount point such as /eos/user. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
