{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f738072",
   "metadata": {},
   "source": [
    "written by Nutchaya Phumekham, July 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a26b6",
   "metadata": {},
   "source": [
    "## Determine the top used datasets by CRAB_UserHN and Workflow frequency. Also show their CpuTimeHr and  Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d5c92",
   "metadata": {},
   "source": [
    "Read the following for reference:   \n",
    "- https://github.com/dmwm/CMSSpark/blob/master/src/python/CMSSpark/rucio_datasets_last_access_ts.py#L88\n",
    "- https://github.com/dmwm/CMSSpark/blob/master/src/python/CMSSpark/rucio_datasets_last_access_ts.py#L36\n",
    "- https://github.com/dmwm/CMSSpark/blob/master/src/python/CMSSpark/schemas.py#L168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e27882a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    _to_dict,\n",
    "    _donut,\n",
    "    _pie,\n",
    "    _line_graph,\n",
    "    _other_fields,\n",
    "    _exitcode_info\n",
    ")\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    lit,\n",
    "    when,\n",
    "    sum as _sum,\n",
    "    count as _count,\n",
    "    first,\n",
    "    date_format,\n",
    "    from_unixtime,\n",
    "    to_date,\n",
    "    countDistinct\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d28153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_schema():\n",
    "    return StructType(\n",
    "        [\n",
    "            StructField(\n",
    "                \"data\",\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"RecordTime\", LongType(), nullable=False),\n",
    "                        StructField(\"InputData\", StringType(), nullable=True),\n",
    "                        StructField(\"Status\", StringType(), nullable=True),\n",
    "                        StructField(\"DESIRED_CMSDataset\", StringType(), nullable=True),\n",
    "                        StructField(\"CpuTimeHr\", DoubleType(), nullable=True),\n",
    "                        StructField(\"RequestCpus\", LongType(), nullable=True),\n",
    "                        StructField(\"GlobalJobId\", StringType(), nullable=False),\n",
    "                        StructField(\"CMS_SubmissionTool\", StringType(), nullable=True),\n",
    "                        StructField(\"CMS_TaskType\", StringType(), nullable=True),\n",
    "                        StructField(\"TaskType\", StringType(), nullable=True),\n",
    "                        StructField(\"CRAB_UserHN\", StringType(), nullable=True),\n",
    "                        StructField(\"USER\", StringType(), nullable=True),\n",
    "                        StructField(\"Workflow\", StringType(), nullable=True)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea489ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_files(start_date, end_date, spark, base):\n",
    "    st_date = start_date - timedelta(days=3)\n",
    "    ed_date = end_date + timedelta(days=3)\n",
    "    days = (ed_date - st_date).days\n",
    "    pre_candidate_files = [\n",
    "        \"{base}/{day}{{,.tmp}}\".format(\n",
    "            base=base, day=(st_date + timedelta(days=i)).strftime(\"%Y/%m/%d\")\n",
    "        )\n",
    "        for i in range(0, days)\n",
    "    ]\n",
    "    sc = spark.sparkContext\n",
    "    candidate_files = [\n",
    "        f\"{base}/{(st_date + timedelta(days=i)).strftime('%Y/%m/%d')}\"\n",
    "        for i in range(0, days)\n",
    "    ]\n",
    "    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "    URI = sc._gateway.jvm.java.net.URI\n",
    "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "    fs = FileSystem.get(URI(\"hdfs:///\"), sc._jsc.hadoopConfiguration())\n",
    "    candidate_files = [url for url in candidate_files if fs.globStatus(Path(url))]\n",
    "    return candidate_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26586807",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DEFAULT_HDFS_FOLDER = \"/project/monitoring/archive/condor/raw/metric\"\n",
    "HDFS_DBS_FILES = '/project/awg/cms/CMS_DBS3_PROD_GLOBAL/current/FILES/part-m-00000'\n",
    "HDFS_DBS_DATASETS = '/project/awg/cms/CMS_DBS3_PROD_GLOBAL/current/DATASETS/part-m-00000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84bff37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = _get_schema()\n",
    "start_date = datetime(2022, 5, 1)\n",
    "end_date = datetime(2022, 5, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14e53b35",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/project/monitoring/archive/condor/raw/metric/2022/04/28',\n",
       " '/project/monitoring/archive/condor/raw/metric/2022/04/29',\n",
       " '/project/monitoring/archive/condor/raw/metric/2022/04/30',\n",
       " '/project/monitoring/archive/condor/raw/metric/2022/05/01',\n",
       " '/project/monitoring/archive/condor/raw/metric/2022/05/02',\n",
       " '/project/monitoring/archive/condor/raw/metric/2022/05/03',\n",
       " '/project/monitoring/archive/condor/raw/metric/2022/05/04',\n",
       " '/project/monitoring/archive/condor/raw/metric/2022/05/05',\n",
       " '/project/monitoring/archive/condor/raw/metric/2022/05/06',\n",
       " '/project/monitoring/archive/condor/raw/metric/2022/05/07',\n",
       " '/project/monitoring/archive/condor/raw/metric/2022/05/08',\n",
       " '/project/monitoring/archive/condor/raw/metric/2022/05/09',\n",
       " '/project/monitoring/archive/condor/raw/metric/2022/05/10']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidate_files(start_date, end_date, spark, base=_DEFAULT_HDFS_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb438a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = (\n",
    "        spark.read.option(\"basePath\", _DEFAULT_HDFS_FOLDER)\n",
    "        .json(\n",
    "            get_candidate_files(start_date, end_date, spark, base=_DEFAULT_HDFS_FOLDER),\n",
    "            schema=schema,\n",
    "        ).select(\"data.*\")\n",
    "        .filter(\n",
    "            f\"\"\"RecordTime >= {start_date.timestamp() * 1000}\n",
    "          AND RecordTime < {end_date.timestamp() * 1000}\n",
    "          \"\"\"\n",
    "        )\n",
    "        .drop_duplicates([\"GlobalJobId\"])\n",
    "    )\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d4cf0",
   "metadata": {},
   "source": [
    "### df1 - time range 05/01 - 05/08 (7days) \n",
    "show _sum(CpuTimeHr), _count(CRAB_UserHN), _count(Workflow) used by each dataset for this week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bf764f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = raw_df.select(col('DESIRED_CMSDataset'),\\\n",
    "                    col('CpuTimeHr'),\\\n",
    "                    col('CRAB_UserHN'),\\\n",
    "                    col('Workflow'))\\\n",
    "            .groupby(col('DESIRED_CMSDataset'))\\\n",
    "            .agg(_sum(\"CpuTimeHr\").alias(\"sum_CpuTimeHr\"),\\\n",
    "                countDistinct(col(\"CRAB_UserHN\")).alias(\"distinct_CRAB_UserHN\"),\n",
    "                countDistinct(col(\"Workflow\")).alias(\"distinct_Workflow\"))\\\n",
    "            .orderBy(col('distinct_CRAB_UserHN').desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a88a6",
   "metadata": {},
   "source": [
    "### df2 - find dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73182575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/dmwm/CMSSpark.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b925902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CMSSpark.src.python.CMSSpark import schemas as cms_schemas\n",
    "csvreader = spark.read.format('csv') \\\n",
    "        .option('nullValue', 'null') \\\n",
    "        .option('mode', 'FAILFAST')\n",
    "dbs_files = csvreader.schema(cms_schemas.schema_files()) \\\n",
    "        .load(HDFS_DBS_FILES) \\\n",
    "        .withColumnRenamed('f_logical_file_name', 'f_name')\n",
    "dbs_datasets = csvreader.schema(cms_schemas.schema_datasets()) \\\n",
    "        .load(HDFS_DBS_DATASETS) \\\n",
    "        .select(['d_dataset_id', 'd_dataset'])\n",
    "df_dbs_f_d = dbs_files.join(dbs_datasets, dbs_files.f_dataset_id == dbs_datasets.d_dataset_id, how='left') \\\n",
    "        .withColumnRenamed('f_dataset_id', 'dataset_id') \\\n",
    "        .withColumnRenamed('d_dataset', 'dataset') \\\n",
    "        .select(['f_name', 'dataset', 'f_file_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a16fc896",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_dbs_f_d.groupby(['dataset'])\\\n",
    "                                .agg(_sum(col('f_file_size')).alias('sum_file_size'))\\\n",
    "                                .orderBy(col('sum_file_size').desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867c6260",
   "metadata": {},
   "source": [
    "### df3 - join df1 and df2 on dataset name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "179cc229",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df1.join(df2, df1.DESIRED_CMSDataset == df2.dataset, how='left')\\\n",
    "        .select(['DESIRED_CMSDataset', 'distinct_CRAB_UserHN', 'distinct_Workflow', 'sum_file_size','sum_CpuTimeHr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e08a7072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DESIRED_CMSDataset: string (nullable = true)\n",
      " |-- distinct_CRAB_UserHN: long (nullable = false)\n",
      " |-- distinct_Workflow: long (nullable = false)\n",
      " |-- sum_file_size: double (nullable = true)\n",
      " |-- sum_CpuTimeHr: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5988cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff04547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
